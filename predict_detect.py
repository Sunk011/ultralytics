# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import datetime
import os

import cv2
import numpy as np
from tqdm import tqdm

from ultralytics import YOLO
from ultralytics.engine.results import Results
from utils.config_loader import Config
from utils.draw_reticle_box import draw_reticle_box
from utils.input_handler import InputStreamHandler


class Yolodetect:
    """
    ä¸€ä¸ªä½¿ç”¨YOLOæ¨¡å‹è¿›è¡Œæ£€æµ‹ã€è£å‰ªå’Œå¯è§†åŒ–çš„å·¥å…·ç±»ã€‚
    ã€ä¼˜åŒ–ç‰ˆã€‘ä½¿ç”¨MMDetectioné£æ ¼è¿›è¡Œç»˜åˆ¶ã€‚.
    """

    def __init__(self, model_path: str = "yolov8n.pt", output_path: str = "./output"):
        print(f"æ­£åœ¨åŠ è½½YOLOæ£€æµ‹æ¨¡å‹: {model_path}")
        try:
            self.model = YOLO(model_path)
            self.class_names = self.model.names
            self.class_name_to_id = {v: k for k, v in self.class_names.items()}
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚å¯æ£€æµ‹ç±»åˆ«: {list(self.class_names.values())}")

            np.random.seed(41)
            self.colors = np.random.randint(0, 255, size=(len(self.class_names), 3), dtype=np.uint8)
            self.output_path = output_path
            self.output_path_crop = os.path.join(self.output_path, "crops")
            self.output_path_draw = os.path.join(self.output_path, "draws")
            self.output_path_video = os.path.join(self.output_path, "videos")
            os.makedirs(self.output_path, exist_ok=True)
            os.makedirs(self.output_path_crop, exist_ok=True)
            os.makedirs(self.output_path_draw, exist_ok=True)
            os.makedirs(self.output_path_video, exist_ok=True)

            self.croped_counter = self._get_dir_file_number(self.output_path_crop)
            self.drawed_counter = self._get_dir_file_number(self.output_path_draw)

            # ä¿å­˜è§†é¢‘ç›¸å…³
            self.video_writer = None
            self.target_frames = 0
            self.video_save_path = ""
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½YOLOæ¨¡å‹ã€‚è¯·æ£€æŸ¥è·¯å¾„å’Œå®‰è£…ã€‚é”™è¯¯è¯¦æƒ…: {e}")
            self.model = None

    def handle_crop_task(self, result, image, target_class_id):
        """å¤„ç†è£å‰ªä»»åŠ¡."""
        cropped_image, status = self.crop_target(result, image, target_class_id)
        if status:
            self.save_result(cropped_image, task="crop_image")

    def handle_draw_task(self, result, image, target_class_id):
        """å¤„ç†ç»˜åˆ¶ä»»åŠ¡."""
        drawed_image, status = self.draw_target(result, image, target_class_id)
        if status:
            self.save_result(drawed_image, task="draw_image")

    def handle_video_task(self, result, image, target_class_id):
        """å¤„ç†è§†é¢‘ä¿å­˜ä»»åŠ¡."""
        drawed_image, _ = self.draw_target(result, image, target_class_id)
        self.save_result(drawed_image, task="save_video", fps=25)

    def handle_display_info(self, result, target_class_id):
        """å¤„ç†æ˜¾ç¤ºä¿¡æ¯."""
        # TODO

    def _get_dir_file_number(self, directory: str) -> int:
        os.makedirs(directory, exist_ok=True)
        max_num = 0
        print(f"æ­£åœ¨æ‰«æç›®å½• '{directory}' ä»¥ç¡®å®šèµ·å§‹ç¼–å·...")
        try:
            for filename in os.listdir(directory):
                if filename.lower().endswith((".jpg", ".png", ".jpeg")):
                    try:
                        # å…¼å®¹ crop_1.jpg å’Œ 1.jpg ä¸¤ç§æ ¼å¼
                        num_str = os.path.splitext(filename)[0]
                        if "_" in num_str:
                            num_str = num_str.split("_")[-1]
                        num = int(num_str)
                        if num > max_num:
                            max_num = num
                    except ValueError:
                        continue
        except Exception as e:
            print(f"æ‰«æç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"æ‰«æå®Œæˆã€‚æœ€å¤§æ–‡ä»¶ç¼–å·ä¸º: {max_num}ã€‚")
        return max_num

    def infer(
        self,
        frame: np.ndarray,
        stream: bool = True,
        conf_threshold: float = 0.5,
        device: str = "0",
        verbose: bool = False,
    ) -> list[Results]:
        results = self.model(frame, stream=stream, conf=conf_threshold, device=device, verbose=verbose)
        return results

    def crop_target(self, det_result: Results, image: np.ndarray, target_class_id: list) -> tuple[np.ndarray, bool]:
        """
        æ ¹æ®å•å¸§çš„YOLOæ¨ç†ç»“æœï¼Œä»åŸå§‹å›¾åƒä¸­è£å‰ªå‡ºæŒ‡å®šç±»åˆ«çš„ç›®æ ‡å›¾åƒã€‚è‹¥æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç›®æ ‡åˆ™è¿”å›è£å‰ªåçš„å›¾åƒï¼Œ
        è‹¥æœªæ‰¾åˆ°åˆ™è¿”å›åŸå›¾ã€‚åŒæ—¶è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼è¡¨ç¤ºæ˜¯å¦æˆåŠŸè£å‰ªã€‚.

        Args:
            det_result (Results): å•å¸§çš„YOLOæ¨ç†ç»“æœå¯¹è±¡
            image (np.ndarray): åŸå§‹çš„OpenCVå›¾åƒå¸§(BGRæ ¼å¼)
            target_class_id (list): è¦è£å‰ªçš„ç›®æ ‡ç±»åˆ«IDåˆ—è¡¨ã€‚-1ä»£è¡¨æ‰€æœ‰ç±»åˆ«

        Returns:
            tuple[np.ndarray, bool]: å…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ ä¸ºè£å‰ªåçš„å›¾åƒï¼Œç¬¬äºŒä¸ªå…ƒç´ è¡¨ç¤ºæ˜¯å¦æˆåŠŸè£å‰ªåˆ°ç›®æ ‡
        """
        boxes = det_result.boxes
        status = False
        for box in boxes:
            class_id = int(box.cls[0])
            if (-1 in target_class_id) or (class_id in target_class_id):
                status = True
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                x1_c, y1_c = max(0, x1), max(0, y1)
                x2_c, y2_c = min(image.shape[1], x2), min(image.shape[0], y2)

                if x1_c < x2_c and y1_c < y2_c:
                    cropped_object = image[y1_c:y2_c, x1_c:x2_c]
                    return cropped_object, status
        return image, status

    def draw_target(
        self, det_result: Results, image: np.ndarray, target_class_id: list = [-1]
    ) -> tuple[np.ndarray, bool]:
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶æŒ‡å®šç±»åˆ«çš„ç›®æ ‡è¾¹ç•Œæ¡†,åŒæ—¶è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼è¡¨ç¤ºæ˜¯å¦æˆåŠŸç»˜åˆ¶ç›®æ ‡ã€‚.

        Args:
            det_result (Results): å•å¸§çš„YOLOæ¨ç†ç»“æœå¯¹è±¡ã€‚
            image (np.ndarray): åŸå§‹çš„OpenCVå›¾åƒå¸§(BGRæ ¼å¼)ã€‚
            target_class_id (list): è¦ç»˜åˆ¶çš„ç›®æ ‡ç±»åˆ«IDåˆ—è¡¨ï¼Œ-1ä»£è¡¨æ‰€æœ‰ç±»åˆ«ã€‚

        Returns:
            tuple[np.ndarray, bool]: å…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ ä¸ºç»˜åˆ¶å¤„ç†åçš„å›¾åƒå¸§ï¼Œç¬¬äºŒä¸ªå…ƒç´ è¡¨ç¤ºæ˜¯å¦æˆåŠŸç»˜åˆ¶ç›®æ ‡ã€‚
        """
        # åˆ›å»ºä¸€ä¸ªç”¨äºç»˜åˆ¶çš„è¦†ç›–å±‚ï¼Œä¸åŸå›¾å¤§å°ç›¸åŒ
        overlay = image.copy()
        final_image = image.copy()  # ç”¨äºæœ€ç»ˆæ··åˆ
        boxes = det_result.boxes
        status = False

        did_draw_anything = False
        for box in boxes:
            class_id = int(box.cls[0])
            should_draw = (-1 in target_class_id) or (class_id in target_class_id)

            if should_draw:
                status = True
                did_draw_anything = True
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                # score = float(box.conf[0])  # ç½®ä¿¡åº¦
                color = self.colors[class_id].tolist()
                # label = f'{class_id}:{self.class_names.get(class_id, f"ID_{class_id}")} {score:.2f}'
                # label = f'{self.class_names.get(class_id, f"ID_{class_id}")} {score:.2f}'
                # print(f"ç»˜åˆ¶ç›®æ ‡: {label} ä½ç½®: ({x1}, {y1}) -> ({x2}, {y2})")

                # --- æ ¸å¿ƒç»˜åˆ¶é€»è¾‘ ---
                # 1. åœ¨è¦†ç›–å±‚ä¸Šç»˜åˆ¶åŠé€æ˜çš„å¡«å……çŸ©å½¢
                alpha = 0.2  # é€æ˜åº¦
                cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)

                # # 2. åœ¨æœ€ç»ˆå›¾åƒä¸Šç»˜åˆ¶ä¸é€æ˜çš„è¾¹ç•Œæ¡†çº¿æ¡ ç»˜åˆ¶æ£€æµ‹æ¡†:çŸ©å½¢
                # cv2.rectangle(final_image, (x1, y1), (x2, y2), color, 2) # çº¿æ¡ç²—ç»†ä¸º2
                # 2.ç»˜åˆ¶ç±»ä¼¼äºç„å‡†æ¡†çš„æ£€æµ‹æ¡†
                final_image = draw_reticle_box(final_image, [x1, y1, x2, y2], percent=0.2, color=color)

                # 3. è®¡ç®—æ–‡æœ¬å¤§å°å¹¶ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                # font_scale = 1 # è°ƒå°å­—ä½“
                # font_thickness = 2
                # (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
                # ç¡®ä¿æ ‡ç­¾èƒŒæ™¯ä¸ä¼šè¶…å‡ºå›¾åƒé¡¶éƒ¨
                # label_y1 = max(y1, text_h + 10) # æ ‡ç­¾æ”¾åœ¨æ¡†çš„å·¦ä¸Šè§’å†…ä¾§æˆ–å¤–ä¾§
                # cv2.rectangle(final_image, (x1, label_y1 - text_h - baseline), (x1 + text_w, label_y1), color, -1)

                # 4. ç»˜åˆ¶æ–‡æœ¬
                # cv2.putText(final_image, label, (x1, label_y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)

        # å¦‚æœè¿›è¡Œäº†ç»˜åˆ¶ï¼Œåˆ™å°†è¦†ç›–å±‚ä¸æœ€ç»ˆå›¾åƒæ··åˆ
        if did_draw_anything:
            final_image = cv2.addWeighted(overlay, alpha, final_image, 1 - alpha, 0)

        return final_image, status

    def save_result(self, posted_image: np.ndarray, task: str = "save_images", fps: int = 25) -> str:
        """
        ä¿å­˜ç»˜åˆ¶åçš„å›¾åƒåˆ°æŒ‡å®šè·¯å¾„ã€‚.

        Args:
            posted_image (np.ndarray): ç»˜åˆ¶åçš„å›¾åƒå¸§ã€‚
            task (str): ä»»åŠ¡ç±»å‹ï¼Œé»˜è®¤'save_images',å¯é€‰ä»»åŠ¡ç±»å‹ï¼š'crop_image','save_image', 'save_video'ã€‚

        Returns:
            str: æˆåŠŸä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¿å­˜å¤±è´¥ã€‚
        """
        # --- ä¿å­˜ ---
        if task == "save_video":
            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
            if not hasattr(self, "video_writer") or self.video_writer is None:
                # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                video_save_path = os.path.join(self.output_path_video, f"video_{timestamp}.mp4")
                self.video_save_path = video_save_path

                # è·å–å›¾åƒå°ºå¯¸å’Œè®¾ç½®è§†é¢‘å‚æ•°
                height, width = posted_image.shape[:2]

                # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
                fourcc = cv2.VideoWriter_fourcc(*"mp4v")
                self.video_writer = cv2.VideoWriter(video_save_path, fourcc, fps, (width, height))
                self.video_frame_count = 0
                print(f"start video: {video_save_path}")

            try:
                # å†™å…¥å½“å‰å¸§
                self.video_writer.write(posted_image)
                self.video_frame_count += 1
            except Exception as e:
                print(f"âŒ è§†é¢‘å†™å…¥é”™è¯¯: {e}")
                self.video_writer.release()
                self.video_writer = None
                return ""

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å½•åˆ¶æ—¶é•¿
            if self.video_frame_count >= (fps * 60):
                self.video_writer.release()
                print(f"âœ… è§†é¢‘å·²æˆåŠŸä¿å­˜åˆ°: {self.output_path_video}")
                # é‡ç½®è§†é¢‘å†™å…¥å™¨çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡å½•åˆ¶
                self.video_writer = None
                # self.is_recording = False  # æ›´æ–°å½•åˆ¶çŠ¶æ€
                return self.video_save_path

            # ä»åœ¨å½•åˆ¶ä¸­ï¼Œè¿”å›å½“å‰è¿›åº¦
            return f"è§†é¢‘å½•åˆ¶ä¸­: {self.video_frame_count}/{self.target_frames} å¸§"
        else:
            if task == "crop_image":
                save_path = self.output_path_crop
                file_num = self.croped_counter
                self.croped_counter += 1
            else:
                save_path = self.output_path_draw
                file_num = self.drawed_counter
                self.drawed_counter += 1
            try:
                filename = f"{task}_{file_num}.jpg"
                save_path = os.path.join(save_path, filename)

                print(f"æ­£åœ¨ä¿å­˜å›¾ç‰‡åˆ°: {save_path}")
                cv2.imwrite(save_path, posted_image)
                print(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {self.output_path}, æ–‡ä»¶å: {filename}")
                return save_path
            except Exception as e:
                print(f"âŒ ä¿å­˜å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯åˆ° '{self.output_path}': {e}")
                return ""


def start(config_path: str = "config.yaml"):
    # åŠ è½½é…ç½®æ–‡ä»¶
    config = Config(config_path)

    # ä»é…ç½®ä¸­è·å–å‚æ•°
    MODEL_PATH = config.get("model.path")
    INPUT_SOURCE = config.get("input.source")
    OUTPUT_PATH = config.get("output.path", "./output")
    TARGET_CLASS_ID = config.get("target_class_id", default=[-1])
    DEVICE = config.get("infer_setting.device", "0")
    CONF = config.get("infer_setting.confidence_threshold", default=0.5)
    TASK_TYPES = config.get("task.type", default="save_video")

    TASK_TYPES = TASK_TYPES.split(",") if hasattr(config, "task") and hasattr(config.task, "type") else []

    print(TASK_TYPES)

    print("=====Start =====")
    from queue import Queue

    yolo_det = Yolodetect(MODEL_PATH, OUTPUT_PATH)

    print(f"ç›®æ ‡ç±»åˆ«ID: {TARGET_CLASS_ID}")

    # ä»»åŠ¡æ˜ å°„å­—å…¸ï¼šå°†ä»»åŠ¡ç±»å‹æ˜ å°„åˆ°å¯¹åº”çš„å¤„ç†å‡½æ•°
    task_handlers = {
        "crop_image": yolo_det.handle_crop_task,
        "draw_image": yolo_det.handle_draw_task,
        "save_video": yolo_det.handle_video_task,
    }
    # æ ¹æ®é…ç½®åˆå§‹åŒ–è¦æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨
    active_tasks = [task_handlers[task.strip()] for task in TASK_TYPES if task.strip() in task_handlers]
    # print(active_tasks)
    # return ''

    camera = InputStreamHandler(INPUT_SOURCE, Queue())
    camera.run()

    print(" ---------- Process Thread Start ---------- ")

    frame_count = 0
    try:
        while True:
            image = camera.frame_queue.get()
            progress_bar = tqdm(desc="å¤„ç†RTSPæµ", unit="frame")  # ä¸è®¾ç½®totalï¼ŒåŠ¨æ€æ˜¾ç¤ºå·²å¤„ç†å¸§æ•°
            progress_bar.update(frame_count)

            frame_count += 1

            # 1. æ¨ç†
            det_results = yolo_det.infer(image, stream=True, conf_threshold=CONF, device=DEVICE)

            for result in det_results:
                # æ‰§è¡Œæ‰€æœ‰æ¿€æ´»çš„ä»»åŠ¡
                for task_handler in active_tasks:
                    task_handler(result, image, TARGET_CLASS_ID)
    except KeyboardInterrupt:
        print("\n =CTRL= KeyboardInterrupt! done ============ ")
    except Exception as e:
        print(f"\n =ERROR= Exception! done ======Reason: {e}")
    finally:
        camera.stop()
        cv2.destroyAllWindows()

    print("\n =^_^= All done, See you ~")


if __name__ == "__main__":
    start(r"./ultralytics/config_yaml/config_detect.yaml")
