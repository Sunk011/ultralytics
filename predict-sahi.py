# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# import os
# import cv2
# import numpy as np
# import torch
# from sahi import AutoDetectionModel
# from sahi.predict import get_sliced_prediction

# class SahiProcessor:
#     """
#     ä¸€ä¸ªä½¿ç”¨YOLOå’ŒSAHIè¿›è¡Œåˆ‡ç‰‡æ¨ç†ã€è£å‰ªå’Œå¯è§†åŒ–çš„å·¥å…·ç±»ã€‚
#     ä¸“ä¸ºä¸å¤šçº¿ç¨‹è§†é¢‘æµå¤„ç†InputStreamHandleré›†æˆè€Œè®¾è®¡ã€‚
#     """
#     def __init__(self, model_path: str, output_path: str = './output'):
#         """
#         åˆå§‹åŒ–å¤„ç†å™¨,ä½¿ç”¨SAHIåŠ è½½YOLOæ¨¡å‹ã€‚

#         Args:
#             model_path (str): æ‚¨è‡ªå®šä¹‰çš„YOLOæ¨¡å‹çš„.ptæ–‡ä»¶è·¯å¾„ã€‚
#             output_path (str): è¾“å‡ºç»“æœçš„ä¿å­˜ç›®å½•ã€‚
#         """
#         print(f"æ­£åœ¨ä½¿ç”¨SAHIåŠ è½½YOLOæ¨¡å‹: {model_path}")
#         try:
#             # æ£€æŸ¥è®¾å¤‡
#             if torch.cuda.is_available():
#                 self.device = torch.device("cuda:0")
#                 print(f"âœ… æ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨è®¾å¤‡: {self.device}")
#             else:
#                 self.device = torch.device("cpu")
#                 print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUã€‚")

#             # ä½¿ç”¨SAHIçš„æ ‡å‡†æ–¹å¼åŠ è½½æ¨¡å‹
#             self.detection_model = AutoDetectionModel.from_pretrained(
#                 model_type='yolov8',
#                 model_path=model_path,
#                 confidence_threshold=0.3, # åˆå§‹é˜ˆå€¼ï¼Œå¯åœ¨æ¨ç†æ—¶è¦†ç›–
#                 device=self.device,
#             )
#             self.class_names = self.detection_model.model.names
#             self.class_name_to_id = {v: k for k, v in self.class_names.items()}
#             print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚å¯æ£€æµ‹ç±»åˆ«: {list(self.class_names.values())}")

#             self.output_path = output_path
#             self.file_counter = self._get_dir_file_number(self.output_path)

#         except Exception as e:
#             print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹ã€‚é”™è¯¯è¯¦æƒ…: {e}")
#             self.detection_model = None

#     def _get_dir_file_number(self, directory: str) -> int:
#         os.makedirs(directory, exist_ok=True)
#         max_num = 0
#         print(f"æ­£åœ¨æ‰«æç›®å½• '{directory}' ä»¥ç¡®å®šèµ·å§‹ç¼–å·...")
#         try:
#             for filename in os.listdir(directory):
#                 if filename.lower().endswith(('.jpg', '.png', '.jpeg')):
#                     try:
#                         num_str = os.path.splitext(filename)[0]
#                         # å…¼å®¹ crop_1.jpg è¿™æ ·çš„æ ¼å¼
#                         if num_str.startswith("crop_"):
#                             num_str = num_str[5:]
#                         num = int(num_str)
#                         if num > max_num: max_num = num
#                     except ValueError: continue
#         except Exception as e: print(f"æ‰«æç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
#         print(f"æ‰«æå®Œæˆã€‚æœ€å¤§æ–‡ä»¶ç¼–å·ä¸º: {max_num}ã€‚")
#         return max_num

#     def infer_with_sahi(self,
#                         frame: np.ndarray,
#                         slice_size: int = 640,
#                         overlap_ratio: float = 0.2,
#                         nms_threshold: float = 0.5,
#                         conf_threshold: float = 0.5) -> list[dict]:
#         """
#         å¯¹è¾“å…¥çš„å›¾åƒå¸§æ‰§è¡ŒSAHIåˆ‡ç‰‡æ¨ç†ã€‚

#         Args:
#             frame (np.ndarray): è¾“å…¥çš„å›¾åƒå¸§ (OpenCV BGRæ ¼å¼)ã€‚
#             slice_size (int): åˆ‡ç‰‡çš„é«˜åº¦å’Œå®½åº¦ã€‚
#             overlap_ratio (float): åˆ‡ç‰‡ä¹‹é—´çš„é‡å ç‡ã€‚
#             conf_threshold (float): ç”¨äºè¿‡æ»¤æœ€ç»ˆç»“æœçš„ç½®ä¿¡åº¦é˜ˆå€¼ã€‚

#         Returns:
#             list[dict]: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ£€æµ‹ç»“æœçš„åˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœæ˜¯ä¸€ä¸ªå­—å…¸ã€‚
#                         e.g., [{'box': [x1,y1,x2,y2], 'score': 0.9, 'class_id': 1, 'class_name': 'bus'}, ...]
#         """
#         if not self.detection_model: return []

#         sahi_result = get_sliced_prediction(
#                 frame,
#                 self.detection_model,
#                 slice_height=slice_size,
#                 slice_width=slice_size,
#                 overlap_height_ratio=overlap_ratio,
#                 overlap_width_ratio=overlap_ratio,
#                 postprocess_match_threshold=nms_threshold, # è¿›è¡Œåå¤„ç†çš„é˜ˆå€¼
#                 verbose= 2
#                 # confidence_threshold=conf_threshold # ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä¿®æ­£å‚æ•°åï¼‰
#             )

#         # å°†SAHIç»“æœè½¬æ¢ä¸ºç®€å•çš„å­—å…¸åˆ—è¡¨
#         predictions_list = []
#         for pred in sahi_result.object_prediction_list:
#             predictions_list.append({
#                 "box": [int(coord) for coord in pred.bbox.to_xyxy()],
#                 "score": pred.score.value,
#                 "class_id": pred.category.id,
#                 "class_name": pred.category.name
#             })
#         return predictions_list

#     def crop_target(self, predictions: list[dict], image: np.ndarray, target_class_id: int) -> None:
#         """
#         ä»é¢„æµ‹ç»“æœåˆ—è¡¨ä¸­è£å‰ªå‡ºæŒ‡å®šç±»åˆ«çš„ç›®æ ‡å¹¶ä¿å­˜ã€‚

#         Args:
#             predictions (list[dict]): æ¥è‡ª infer_with_sahi æ–¹æ³•çš„é¢„æµ‹ç»“æœåˆ—è¡¨ã€‚
#             image (np.ndarray): åŸå§‹çš„ã€æœªè¢«ä¿®æ”¹çš„å›¾åƒå¸§ã€‚
#             target_class_id (int): è¦è£å‰ªçš„ç›®æ ‡çš„ç±»åˆ«IDã€‚
#         """
#         image_copy = image.copy()
#         for pred in predictions:
#             if pred['class_id'] == target_class_id:
#                 x1, y1, x2, y2 = pred['box']
#                 x1_c, y1_c = max(0, x1), max(0, y1)
#                 x2_c, y2_c = min(image_copy.shape[1], x2), min(image_copy.shape[0], y2)

#                 if x1_c < x2_c and y1_c < y2_c:
#                     cropped_object = image_copy[y1_c:y2_c, x1_c:x2_c]
#                     self.file_counter += 1
#                     filename = f"crop_{self.file_counter}.jpg"
#                     save_path = os.path.join(self.output_path, filename)
#                     cv2.imwrite(save_path, cropped_object)
#                     print(f"âœ‚ï¸ å·²è£å‰ªå›¾ç‰‡æ•°é‡: {self.file_counter}", end='\r')


# import os
# from queue import Queue
# from tqdm import tqdm
# from utils.input_handler import InputStreamHandler

# if __name__ == '__main__':
#     # 1. --- å‚æ•°é…ç½® ---
#     MODEL_PATH = r'./jg_project/v001-epoch100-aug/weights/best.pt'
#     INPUT_SOURCE = r'./datasets/test-video/mp4/343205-2025v07201624-202507201629.mp4'
#     # INPUT_SOURCE = 'rtsp://your_rtsp_url'
#     OUTPUT_PATH = r'./output_sahi_crops'

#     # --- SAHI å’Œ æ¨ç†å‚æ•° ---
#     SLICE_SIZE = 640
#     OVERLAP_RATIO = 0.2
#     NMS_THRESHOLD = 0.5
#     CONF_THRESHOLD = 0.45
#     DEVICE = '0' # SAHIä¼šè‡ªåŠ¨å¤„ç†è®¾å¤‡ï¼Œè¿™é‡Œä¿ç•™ç»™æœªæ¥å¯èƒ½çš„æ‰©å±•

#     # --- è£å‰ªç›®æ ‡å‚æ•° ---
#     # ã€æ³¨æ„ã€‘è¿™é‡Œä½¿ç”¨ç±»åˆ«åç§°ï¼Œæ›´å‹å¥½ã€‚å¦‚æœæ¨¡å‹ä¸­æ²¡æœ‰è¯¥ç±»åˆ«ï¼Œç¨‹åºä¼šæŠ¥é”™å¹¶é€€å‡ºã€‚
#     TARGET_CLASS_NAME = 'bus'

#     print("===== å¼€å§‹åˆå§‹åŒ– =====")

#     # 2. --- åˆå§‹åŒ–å¤„ç†å™¨å’Œè§†é¢‘æµ ---
#     # åˆå§‹åŒ–SAHIå¤„ç†å™¨
#     sahi_proc = SahiProcessor(MODEL_PATH, OUTPUT_PATH)

#     # æ£€æŸ¥ç›®æ ‡ç±»åˆ«æ˜¯å¦å­˜åœ¨
#     if TARGET_CLASS_NAME not in sahi_proc.class_name_to_id:
#         print(f"âŒ é”™è¯¯: ç›®æ ‡ç±»åˆ« '{TARGET_CLASS_NAME}' ä¸å­˜åœ¨äºæ¨¡å‹ä¸­ã€‚")
#         print(f"å¯ç”¨ç±»åˆ«: {list(sahi_proc.class_names.values())}")
#         exit()
#     target_class_id = sahi_proc.class_name_to_id[TARGET_CLASS_NAME]

#     print('---------- start ----------')
#     # åˆå§‹åŒ–è§†é¢‘æµå¤„ç†å™¨
#     frame_queue = Queue()
#     camera = InputStreamHandler(INPUT_SOURCE, frame_queue)
#     camera.run()

#     print(' ---------- SAHI Process Thread Start ---------- ')
#     frame_count = 0
#     try:
#         while True:
#             # ä»é˜Ÿåˆ—ä¸­è·å–åŸå§‹å¸§
#             image = frame_queue.get()
#             # if image is None: # å¯ä»¥çº¦å®šä¸€ä¸ªNoneä½œä¸ºç»“æŸä¿¡å·
#             #     break
#             frame_count += 1

#             # --- æ ¸å¿ƒå¤„ç†æ­¥éª¤ ---
#             # 1. ä½¿ç”¨SAHIè¿›è¡Œåˆ‡ç‰‡æ¨ç†ï¼Œè·å–é¢„æµ‹ç»“æœåˆ—è¡¨
#             predictions = sahi_proc.infer_with_sahi(
#                 frame=image,
#                 slice_size=SLICE_SIZE,
#                 overlap_ratio=OVERLAP_RATIO,
#                 nms_threshold = NMS_THRESHOLD,
#               # conf_threshold=CONF_THRESHOLD
#             )

#             # 2. å¦‚æœæœ‰é¢„æµ‹ç»“æœï¼Œåˆ™è°ƒç”¨è£å‰ªå‡½æ•°
#             if predictions:
#                 sahi_proc.crop_target(
#                     predictions=predictions,
#                     image=image,
#                     target_class_id=target_class_id
#                 )

#     except KeyboardInterrupt:
#         print("\n =CTRL= æ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­! æ­£åœ¨é€€å‡º...")
#     except Exception as e:
#         print(f"\n =ERROR= å‘ç”Ÿå¼‚å¸¸! åŸå› : {e}")
#     finally:
#         # ç¡®ä¿çº¿ç¨‹è¢«å®‰å…¨åœæ­¢
#         camera.stop()
#     print(' =^_^= All done, See you ~')


import os

import cv2
import numpy as np
import torch
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction


class SahiProcessor:
    """
    ä¸€ä¸ªä½¿ç”¨YOLOå’ŒSAHIè¿›è¡Œåˆ‡ç‰‡æ¨ç†ã€è£å‰ªå’Œå¯è§†åŒ–çš„å·¥å…·ç±»ã€‚
    ã€å¤šç›®æ ‡ç±»åˆ«å¤„ç†ç‰ˆã€‘.
    """

    def __init__(self, model_path: str, output_path: str = "./output", device: str = "0"):
        print(f"æ­£åœ¨ä½¿ç”¨SAHIåŠ è½½YOLOæ¨¡å‹: {model_path}")
        try:
            if torch.cuda.is_available():
                self.device = torch.device("cuda:0")
                print(f"âœ… æ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨è®¾å¤‡: {self.device}")
            else:
                self.device = torch.device("cpu")
                print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUã€‚")

            self.detection_model = AutoDetectionModel.from_pretrained(
                model_type="yolov8",
                model_path=model_path,
                confidence_threshold=0.3,
                device=self.device,
            )
            self.class_names = self.detection_model.model.names
            self.class_name_to_id = {v: k for k, v in self.class_names.items()}
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚å¯æ£€æµ‹ç±»åˆ«: {list(self.class_names.values())}")

            # --- ä¸ºç»˜åˆ¶åŠŸèƒ½åˆå§‹åŒ–é¢œè‰² ---
            np.random.seed(42)
            self.colors = np.random.randint(0, 255, size=(len(self.class_names), 3), dtype=np.uint8)

            self.output_path = output_path
            self.file_counter = self._get_dir_file_number(self.output_path)

        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½æ¨¡å‹ã€‚é”™è¯¯è¯¦æƒ…: {e}")
            self.detection_model = None

    def _get_dir_file_number(self, directory: str) -> int:
        os.makedirs(directory, exist_ok=True)
        max_num = 0
        print(f"æ­£åœ¨æ‰«æç›®å½• '{directory}' ä»¥ç¡®å®šèµ·å§‹ç¼–å·...")
        try:
            for filename in os.listdir(directory):
                if filename.lower().endswith((".jpg", ".png", ".jpeg")):
                    try:
                        num_str = os.path.splitext(filename)[0]
                        if num_str.startswith("crop_") or num_str.startswith("draw_"):
                            num_str = num_str[5:]
                        num = int(num_str)
                        if num > max_num:
                            max_num = num
                    except ValueError:
                        continue
        except Exception as e:
            print(f"æ‰«æç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"æ‰«æå®Œæˆã€‚æœ€å¤§æ–‡ä»¶ç¼–å·ä¸º: {max_num}ã€‚")
        return max_num

    def infer_with_sahi(
        self, frame: np.ndarray, slice_size: int = 640, overlap_ratio: float = 0.2, conf_threshold: float = 0.5
    ) -> list[dict]:
        if not self.detection_model:
            return []
        sahi_result = get_sliced_prediction(
            frame,
            self.detection_model,
            slice_height=slice_size,
            slice_width=slice_size,
            overlap_height_ratio=overlap_ratio,
            overlap_width_ratio=overlap_ratio,
            postprocess_match_threshold=conf_threshold,
            # postprocess_class_threshold=conf_threshold
            verbose=2,
        )

        predictions_list = []
        for pred in sahi_result.object_prediction_list:
            predictions_list.append(
                {
                    "box": [int(coord) for coord in pred.bbox.to_xyxy()],
                    "score": pred.score.value,
                    "class_id": pred.category.id,
                    "class_name": pred.category.name,
                }
            )
        print(f"SAHI æ¨ç†å®Œæˆã€‚æ£€æµ‹åˆ° {len(predictions_list)} ä¸ªç›®æ ‡ã€‚")
        print(f"æ£€æµ‹åˆ°çš„ç›®æ ‡: {predictions_list}")
        return predictions_list

    def crop_targets(self, predictions: list[dict], image: np.ndarray, target_class_ids: list[int]) -> None:
        """
        ä»é¢„æµ‹ç»“æœåˆ—è¡¨ä¸­è£å‰ªå‡ºæŒ‡å®šç±»åˆ«åˆ—è¡¨ä¸­çš„ç›®æ ‡å¹¶ä¿å­˜ã€‚.

        Args:
            predictions (list[dict]): æ¥è‡ª infer_with_sahi æ–¹æ³•çš„é¢„æµ‹ç»“æœåˆ—è¡¨ã€‚
            image (np.ndarray): åŸå§‹çš„ã€æœªè¢«ä¿®æ”¹çš„å›¾åƒå¸§ã€‚
            target_class_ids (list[int]): è¦è£å‰ªçš„ç›®æ ‡çš„ç±»åˆ«IDåˆ—è¡¨ã€‚å¦‚æœä¸º[-1], åˆ™è£å‰ªæ‰€æœ‰æ£€æµ‹åˆ°çš„ç›®æ ‡ã€‚
        """
        should_crop_all = -1 in target_class_ids

        for pred in predictions:
            # æ£€æŸ¥å½“å‰é¢„æµ‹çš„ç±»åˆ«æ˜¯å¦åœ¨ç›®æ ‡åˆ—è¡¨ä¸­ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦è£å‰ªæ‰€æœ‰
            if should_crop_all or pred["class_id"] in target_class_ids:
                x1, y1, x2, y2 = pred["box"]
                x1_c, y1_c = max(0, x1), max(0, y1)
                x2_c, y2_c = min(image.shape[1], x2), min(image.shape[0], y2)

                if x1_c < x2_c and y1_c < y2_c:
                    cropped_object = image[y1_c:y2_c, x1_c:x2_c]
                    self.file_counter += 1
                    filename = f"crop_{self.file_counter}.jpg"
                    save_path = os.path.join(self.output_path, filename)
                    cv2.imwrite(save_path, cropped_object)
                    print(f"âœ‚ï¸ å·²è£å‰ªå›¾ç‰‡æ•°é‡: {self.file_counter}", end="\r")

    def draw_targets(self, predictions: list[dict], image: np.ndarray, target_class_ids: list) -> np.ndarray:
        """
        æ ¹æ®é¢„æµ‹ç»“æœåœ¨å›¾åƒä¸Šç»˜åˆ¶æŒ‡å®šç±»åˆ«åˆ—è¡¨ä¸­çš„è¾¹ç•Œæ¡†ã€‚.

        Args:
            predictions (list[dict]): æ¥è‡ª infer_with_sahi æ–¹æ³•çš„é¢„æµ‹ç»“æœåˆ—è¡¨ã€‚
            image (np.ndarray): åŸå§‹çš„OpenCVå›¾åƒå¸§ï¼ˆBGRæ ¼å¼ï¼‰ã€‚
            target_class_ids (list): è¦ç»˜åˆ¶çš„ç›®æ ‡ç±»åˆ«IDåˆ—è¡¨ã€‚å¦‚æœä¸º[-1], åˆ™ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„ç›®æ ‡ã€‚

        Returns:
            np.ndarray: ç»è¿‡ç»˜åˆ¶å¤„ç†åçš„æ–°å›¾åƒå¸§ã€‚
        """
        display_image = image.copy()
        should_draw_all = -1 in target_class_ids
        print(f"target_class_ids: {target_class_ids}___{should_draw_all}")

        for pred in predictions:
            # æ£€æŸ¥å½“å‰é¢„æµ‹çš„ç±»åˆ«æ˜¯å¦åœ¨ç›®æ ‡åˆ—è¡¨ä¸­ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦ç»˜åˆ¶æ‰€æœ‰
            if should_draw_all or pred["class_id"] in target_class_ids:
                box = pred["box"]
                score = pred["score"]
                class_id = pred["class_id"]
                class_name = pred["class_name"]

                x1, y1, x2, y2 = box
                color = self.colors[class_id].tolist()
                label = f"ID_{class_id}:{class_name} {score:.2f}"
                print(f"ç»˜åˆ¶: {label}")

                cv2.rectangle(display_image, (x1, y1), (x2, y2), color, 2)
                (text_w, text_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
                cv2.rectangle(display_image, (x1, y1 - text_h - baseline), (x1 + text_w, y1), color, -1)
                cv2.putText(
                    display_image, label, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2
                )

        # return display_image
        # --- ä¿å­˜ ---
        try:
            # output_dir = os.path.dirname(self.output_path)
            # if output_dir:
            print(f"output_path: {self.output_path}")
            os.makedirs(self.output_path, exist_ok=True)
            self.file_counter += 1
            filename = f"draw_{self.file_counter}.jpg"
            save_path = os.path.join(self.output_path, filename)

            print(f"æ­£åœ¨ä¿å­˜å›¾ç‰‡åˆ°: {save_path}")
            cv2.imwrite(save_path, display_image)
            print(f"âœ… ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {self.output_path}")  # å¯ä»¥é€‰æ‹©æ€§åœ°ä¿ç•™æ­¤æ‰“å°è¯­å¥
            return save_path
        except Exception as e:
            print(f"âŒ ä¿å­˜å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯åˆ° '{self.output_path}': {e}")
            return ""


from queue import Queue

from utils.input_handler import InputStreamHandler  # å‡è®¾è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

# from sahi_processor import SahiProcessor

if __name__ == "__main__":
    # 1. --- å‚æ•°é…ç½® ---
    MODEL_PATH = r"./jg_project/v001-epoch100-aug/weights/best.pt"  # æ‚¨çš„æ£€æµ‹æ¨¡å‹
    # INPUT_SOURCE = r'./datasets/test-video/mp4/your_video.mp4' # æ‚¨çš„è§†é¢‘æº
    INPUT_SOURCE = r"./datasets/test-video/test0/mp4/343205-202507201624-202507201629.mp4"

    OUTPUT_PATH = r"./output_sahi_draw"
    # OUTPUT_PATH = r'./output_sahi_crop'
    DEVICE = "3"

    # --- SAHI å’Œ æ¨ç†å‚æ•° ---
    SLICE_SIZE = 640
    OVERLAP_RATIO = 0.2
    CONF_THRESHOLD = 0.45

    # --- åŠŸèƒ½æ§åˆ¶å‚æ•° ---
    # ã€æ–°ã€‘è¦è£å‰ªçš„ç›®æ ‡ç±»åˆ«IDåˆ—è¡¨ã€‚[-1]ä»£è¡¨æ‰€æœ‰ã€‚
    TARGET_IDS_TO_CROP = [1]  # ä¾‹å¦‚ï¼Œåªè£å‰ªIDä¸º1çš„ 'bus'
    # TARGET_IDS_TO_CROP = [0, 1] # ä¾‹å¦‚ï¼Œè£å‰ªIDä¸º0çš„ 'car' å’Œ IDä¸º1çš„ 'bus'
    # TARGET_IDS_TO_CROP = [-1] # è£å‰ªæ‰€æœ‰æ£€æµ‹åˆ°çš„ç‰©ä½“

    # ã€æ–°ã€‘è¦ç»˜åˆ¶çš„ç›®æ ‡ç±»åˆ«IDåˆ—è¡¨ã€‚[-1]ä»£è¡¨æ‰€æœ‰ã€‚
    # TARGET_IDS_TO_DRAW = [-1] # åœ¨è§†é¢‘ä¸Šç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„ç‰©ä½“
    TARGET_IDS_TO_DRAW = [1]  # åªåœ¨è§†é¢‘ä¸Šç»˜åˆ¶IDä¸º1çš„ 'bus'

    DISPLAY_WINDOW = False  # æ˜¯å¦å®æ—¶æ˜¾ç¤ºå¸¦æ¡†çš„è§†é¢‘

    # 2. --- åˆå§‹åŒ–å¤„ç†å™¨å’Œè§†é¢‘æµ ---
    print("===== å¼€å§‹åˆå§‹åŒ– =====")
    sahi_proc = SahiProcessor(MODEL_PATH, OUTPUT_PATH, DEVICE)

    frame_queue = Queue()
    camera = InputStreamHandler(INPUT_SOURCE, frame_queue)
    camera.run()

    print("---------- å¼€å§‹å¤„ç†å¾ªç¯ ----------")
    try:
        while True:
            image = frame_queue.get()
            # if image is None: break

            # 1. ä½¿ç”¨SAHIè¿›è¡Œåˆ‡ç‰‡æ¨ç†
            predictions = sahi_proc.infer_with_sahi(
                frame=image, slice_size=SLICE_SIZE, overlap_ratio=OVERLAP_RATIO, conf_threshold=CONF_THRESHOLD
            )

            # 2. å¦‚æœæœ‰é¢„æµ‹ç»“æœï¼Œåˆ™æ‰§è¡Œè£å‰ª
            # if predictions and TARGET_IDS_TO_CROP:
            #     sahi_proc.crop_targets(
            #         predictions=predictions,
            #         image=image,
            #         target_class_ids=TARGET_IDS_TO_CROP
            #     )
            if predictions and TARGET_IDS_TO_DRAW:
                sahi_proc.draw_targets(predictions=predictions, image=image, target_class_ids=TARGET_IDS_TO_DRAW)

            # # 3. å¦‚æœéœ€è¦æ˜¾ç¤ºçª—å£ï¼Œåˆ™æ‰§è¡Œç»˜åˆ¶
            # if DISPLAY_WINDOW:
            #     annotated_frame = sahi_proc.draw_targets(
            #         predictions=predictions,
            #         image=image,
            #         target_class_ids=TARGET_IDS_TO_DRAW
            #     )
            #     cv2.imshow("SAHI Multi-Target Inference", annotated_frame)
            #     if cv2.waitKey(1) & 0xFF == ord('q'):
            #         print("\nç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢ã€‚")
            #         break

    except KeyboardInterrupt:
        print("\n =CTRL= æ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­! æ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"\n =ERROR= å‘ç”Ÿå¼‚å¸¸! åŸå› : {e}")
    finally:
        camera.stop()
        # if DISPLAY_WINDOW:
        #     cv2.destroyAllWindows()

    print("\n =^_^= å…¨éƒ¨å®Œæˆ, å†è§~")
